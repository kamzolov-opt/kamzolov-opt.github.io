@article{sadiev2024stochastic,
   abstract = {This work considers the non-convex finite-sum minimization problem. There are several algorithms for such problems, but existing methods often work poorly when the problem is badly scaled and/or ill-conditioned, and a primary goal of this work is to introduce methods that alleviate this issue. Thus, here we include a preconditioner based on Hutchinson’s approach to approximating the diagonal of the Hessian and couple it with several gradient-based methods to give new ‘scaled’ algorithms: Scaled SARAH and Scaled L-SVRG. Theoretical complexity guarantees under smoothness assumptions are presented. We prove linear convergence when both smoothness and the PL-condition are assumed. Our adaptively scaled methods use approximate partial second-order curvature information and, therefore, can better mitigate the impact of badly scaled problems. This improved practical performance is demonstrated in the numerical experiments also presented in this work.},
   author = {Abdurakhmon Sadiev and Aleksandr Beznosikov and Abdulla Jasem Almansoori and Dmitry Kamzolov and Rachael Tappenden and Martin Takáč},
   doi = {10.1007/s10957-023-02365-3},
   issn = {1573-2878},
   issue = {2},
   journal = {Journal of Optimization Theory and Applications},
   pages = {471-489},
   title = {Stochastic Gradient Methods with Preconditioned Updates},
   volume = {201},
   url = {https://doi.org/10.1007/s10957-023-02365-3},
   year = {2024},
}